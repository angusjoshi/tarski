\section{Shapley's Stochastic Games} \label{shapleyChap}
Shapley's stochastic games, or stopping stochastic games as described in \citep{shapley}
are a class of zero-sum game played on a set of states with two players
called the maximizer and minimizer respectively.
At each state the players concurrently choose an action,  
and then receive a payoff which sums to zero based on the joint
action of the two players. The game then halts with some fixed probability.
If it didn't halt then a next state is chosen randomly with probability distribution
dependent on the joint action chosen.
It is shown in \citep{shapley} that stochastic games necessarily have an optimal expected value,
and further that this value can be achieved in stationary strategies. Unlike
simple stochastic games however, the value need not be rational or the strategies
achieving this value pure. In \citep{lowerBound} it is shown that the  problem
of finding a rational number $\varepsilon$ close to the actual value of the game
is polynomial-time reducible to $\trsk$ which will be described in this section.
\begin{figure}[h]
  \centering
  \input{diagrams/shapley-diagram.tex}
  \caption{An instance of shapley's stochastic games is a set of states,
  and for each state a pair of actions sets for the two players.
  Each player picks an action, and receives payoff equal to the entry in
  the payoff matrix (in the top half of each circle) corresponding
  to the joint action. Then play transitions transitions to a new state 
  on a probability distribution 
  according to the transition matrix (in the bottom of each circle). In the
  stopping variant, after each step the games halts with some
  positive probability $q \in \rpos$.} 
\end{figure}
\begin{definition}[Stopping Stochastic Game]
  A \emph{stopping stochastic game} $G = (V, A, P, s, q)$ is a set of states of $n$ states
  $V = (v_1, ..., v_n)$, for each state $v_i$ an $n_i \times m_i$ rational valued matrix $A^i \in A$ called
  the \emph{payoff matrix}, for each state $v_i$ an $n_i \times m_i$ matrix $P^i \in P$ called
  the \emph{transition matrix} who's $j, k$-th entry is an $n$-vector 
  $P_{j, k}^i = \left((P_{j, k}^i)_1, ..., (P_{j, k}^i)_n \right)$
  such that $\sum_{l \in [n]} (P_{j, k}^i)_l = 1$ and for each $l \in [n]$, $(P_{j, k}^i)_l \geq 0$.
  The \emph{starting state} is a state $s \in V$ and the \emph{stopping probability} is a positive
  rational number $q \in \Q_{> 0}$. A \emph{play} is as follows. A token is placed on the initial
  state $s = v_i$. The maximizer and minimizer choose actions $j \in [n_i]$ and $k \in [m_i]$ respectively,
  and receive payoffs $p_{\max} = A_{j, k}^i$ and $p_{\min} = -A_{j, k}^i$ respectively. The game then
  halts with probability $q$, and if it did not transitions to state $l \in [n]$ with probability
  $(P_{j, k}^i)_l$. Play continues until it halts (which happens with probability 1).
\end{definition}
We require a more general notion of strategy to the pure strategies introduced in \cref{ssgs} which
are as follows.
\begin{definition}[Mixed Stationary Strategy]
  Let $G = (V, A, P, s, q)$ be a stopping stochastic game. A \emph{mixed stationary strategy}
  for the maximizer is a vector of rational vectors 
  $\sigma = ((\sigma_1^{1}, ..., \sigma_1^{n_1}), ..., ((\sigma_n^{1}, ..., \sigma_n^{n_n})))$ such
  that for each $i \in [n]$, $\sum_{j \in [n_i]} \sigma_i^j = 1$ and for each $j \in [n_i]$, $\sigma_i^j \geq 0$.
  The set of all mixed stationary strategies for the maximizer is denoted $S$.
  A mixed stationary strategy for the minimizer is 
  $\tau = ((\tau_1^{1}, ..., \tau_1^{m_1}), ..., (\tau_n^{1}, ..., \tau_n^{m_n}))$
  such that for each $i \in [n]$, $\sum_{j \in [m_i]} \tau_i^j = 1$ 
  and for each $j \in [m_i]$, $\tau_i^j \geq 0$.
  The set of all mixed stationary strategies for the minimizer is denoted $T$.
\end{definition}
Once a stationary strategy profile is fixed, given a starting state, 
I can define a sequence of random variables
$(X_t)_{t \in \znn}$ representing the payoff at the $t$-th step, allowing for the convention
that the game continues to 'play' after halting, just giving both players payoff 0 repeatedly. The value of the game
then becomes the (necessarily convergent) series $\sum_{t = 0}^\infty \E[X_t]$. More detail on this can be found in
\citep{compMdps}. Then the value of a state $x \in V$ 
under stationay strategy profile $(\sigma, \tau)$ is denoted $v_{\sigma, \tau}(x)$. The content of the following
result from \citep{shapley} is that stochastic games necessarily have a minimax value, and that value can be achieved
in stationary strategies.
\begin{prop}[\citep{shapley}]
  Let $G = (V, A, P, s, q)$ be a stopping stochastic game and $S$, $T$ be the mixed stationary strategy
  sets for the maximizer and minimizer respectively. Then for all $x \in V$, 
  \begin{align*}
    \max_{\sigma \in S} \min_{\tau \in T} v_{\sigma, \tau}(x) = \min_{\tau \in T} \max_{\sigma \in S} v_{\sigma, \tau}(x).
  \end{align*}
\end{prop}
The value of the game is then denoted $v(s)$. The value of a particular state $x \in V$ is denoted $v(x)$
and is the maximum expected value if the game were to start at $x$.
Recall the definition of $\val$ from \cref{matrixGameVal}.
\begin{definition}[Stopping Stochastic Game Monotone Function] \label{shapleyMonotone}
  Let $G = (V, A, P, s, q)$ be a stopping stochastic game, $d = |V|$ and $M = \max_{i, j, k} |A_{j, k}^i|$. Then the
  \emph{stopping stochastic game monotone function} is a function
  $F : [-\frac{M}{q}, \frac{M}{q}]^d \to [-\frac{M}{q}, \frac{M}{q}]^d$ defined coordinatewise as
  $F(x)_i =  \val \left( A^i + (1 - q) \cdot T^i \right)$ where $T^i$ is an $n_i \times m_i$ matrix defined
  $T^i_{j, k} = \sum_{l = 1}^n (P_{j, k}^i)_l \cdot x_l$.
\end{definition}
I was perhaps hasty in writing the codomain of this function, but the following lemma
shows it to be correct.
\begin{lemma}
  Let $G = (V, A, P, s, q)$ be a stopping stochastic game and $F$ as defined in \cref{shapleyMonotone}.
  Then for all $x \in [-\frac{M}{q}, \frac{M}{q}]^d$, $F(x) \in [-\frac{M}{q}, \frac{M}{q}]^d$.
  Further, $F$ is monotone.
\end{lemma}
\newcommand{\mat}[1]{\left\llbracket #1 \right\rrbracket}
\begin{proof}
  The first observation is that the $\val(\cdot)$ operator is monotone in the coordinatewise ordering
  for the corresponding set of matrices. For
  if $\val(B) = k$ for some $B \in \mathbf{Mat}(\Q, n \times m)$ then there is some
  distribution $x \in \Q^n$ such that for all distributions $y \in \Q^m$, $x^T B y \geq k$.
  Then if $B \leq B' \in \mathbf{Mat}(\Q, n \times m)$ then since $x$ and $y$ necessarily have
  all non-negative entries, with $x$ and $y$ quantified similarly, $k \leq x^T B y \leq x^T B' y$. So $\val(B) \leq \val(B')$.
  It also follows easily at this point that $F$ is montone, for all multiplicative factors involved can be seen to be
  non-negative.
  Secondly, if $N = \max_{j, k} |B_{j, k}|$ then clearly the maximizer can achieve payoff
  no larger than the largest entry in the matrix, and no smaller than the smallest. So $-N \leq \val(B) \leq N$.
  Now by monotonicity I have for all $x \in [-\frac{M}{q}, \frac{M}{q}]^d$, $F(x) \leq F\left(\vec{\frac{M}{q}}\right)$.
  I take some liberty in denoting $\mat{k}$ to be the matrix
  with entries all equal to $k$ with size inferred from context. For each $i \in [d]$,
  \begin{align*}
    F\left(\vec{\frac{M}{q}}\right)_i &= \val(A^i + (1 - q) \cdot T^i) \\
                               &\leq \val\left(\mat{M} + (1 - q) \mat{\frac{M}{q}}\right) \\
                               &= \val\left(\mat{M} - \mat{M} + \mat{\frac{M}{q}}\right) = \frac{M}{q}.
  \end{align*}
  Noting that in the case $x = \vec{\frac{M}{q}}$, $T^i$ can clearly be seen to be equal to $\mat{\frac{M}{q}}$ for
  each $i \in [d]$.
  It can be shown using a similar argument that $F(x) \geq -\frac{M}{q}$ by observing again by monotonicity
  that $F(x) \geq F\left(-\frac{M}{q}\right)$.
\end{proof}
Shapley shows in \citep{shapley} that for all $x, x' \in [-\frac{M}{q}, \frac{M}{q}]^d$ 
that $\|F(x) - F(x)'\|_\infty \leq (1 - q) \|x - x'\|_\infty$,
which is to say $F$ is a contraction map and has a \emph{unique} fixpoint by the banach fixpoint theorem. 
It is further shown in \citep{shapley} the unique fixpoint gives precisely the value of the game. That is, 
if $x^* = F(x^*)$ then
for all $i \in [d]$, $v(v_i) = x^*_i$. The last step is a discretization. I
use the notation $\langle M \rangle = \{-M, ..., 0, ..., M\}$. 
\newcommand{\angm}{\langle M \rangle}
\begin{lemma}[Stopping Stochastic Game Discretization] \label{shapleyDiscretization}
  Let $G = (V, A, P, s, q)$ be a stopping stochastic game, $F$ as defined in \cref{shapleyMonotone}, 
  and $\varepsilon \in \rpos$. Then if $M$ is the maximum entry in the set of payoff matrices, let 
  $N = \frac{M}{\varepsilon q^2}$ and define a function $f : \langle N \rangle \to \langle N \rangle$ coordinatewise with
  $f(x)_i = \left\lfloor \frac{1}{\varepsilon q} F(\varepsilon q \cdot x)_i \right\rfloor$. Then if $f(x) = x$
  and $x^*$ is the unique fixpoint of $F$, $\|\varepsilon q \cdot x - x^*\|_\infty$.
\end{lemma}
\begin{proof}
  From $f(x) = x$ I have by definition of $\lfloor \cdot \rfloor$ for each $i \in [d]$,
  $1 > \frac{1}{\varepsilon q} \cdot F(\varepsilon q \cdot x)_i - x_i \geq 0$, which is to say
  $\|\frac{1}{\varepsilon q} \cdot F(\varepsilon q \cdot x) - x\|_\infty < 1$. Then by homogeneity of the norm,
  $\|F(\varepsilon q \cdot x) - x\|_\infty < \varepsilon q$. I calculate,
  \begin{align*}
    \|\varepsilon q \cdot x - x^* \|_\infty &\leq \|\varepsilon q \cdot x - F(\varepsilon q \cdot x) \|_\infty
                                           + \| F(\varepsilon q \cdot x) - x^* \|_\infty \\
                                      &=  \|\varepsilon q x - F(\varepsilon q \cdot x) \|_\infty
                                        +  \| F(\varepsilon q \cdot x) - F(x^*) \|_\infty \\
                                      &< \varepsilon q + (1 - q)\| \varepsilon q \cdot x - x^* \|_\infty.
  \end{align*}
  Rearranging and dividing through by $q$ gives the claim.
\end{proof}
It should be noted that in \citep[Proposition 6.2.]{lowerBound} a similar result to \cref{shapleyDiscretization} 
is proven. There is however a mistake at the end of the proof in defining the discretized function, which the above
shows how to rectify.
I leave out details on encoding sizes, but these can be found in \citep{lowerBound}.
Putting it all together,
\begin{theorem}[\citep{lowerBound}]
  Let $G$ be a stopping stochastic game and $v \in \R$ the value of the game. Then for all $\varepsilon \in \Q_{> 0}$
  computing 
  an $x \in \Q$ such that $|x - v| < \varepsilon$ is polynomial-time reducible to $\trsk$ in the encoding size
  of $\varepsilon$ and $G$.
\end{theorem}
